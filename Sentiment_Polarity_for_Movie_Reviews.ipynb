{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04ELlyEc_ENn"
      },
      "source": [
        "---\n",
        "\n",
        "# The Learning Framework:\n",
        "\n",
        "The model we will use is a simple hyperplane. This plane will represent the decision boundary through the data space, separating positive from negative ratings.\n",
        "\n",
        "1. __Hyperplane model__. The model should be a hyperplane as $f(x, \\omega) = sgn(\\omega^\\top x)$, where $sgn(\\cdot)$ is the sign function. Note that $x_0$ in this notation is the pseudo input 1. When evaluated, this gives us the predicted results as $\\hat y_i = f(X_i, \\omega^{(t)})$, where $\\omega^{(t)}$ is the parameter vector at optimization iteration t and $\\omega^{(0)}$ is the initial guess for the parameter vector.\n",
        "\n",
        "2. __Objective function__. The loss function for our model is the hinge loss, which will be used together with $l_2$ regularization. <br> <br> $\\mathfrak{L}(X, y, \\omega) = \\frac{\\lambda}{2} ||\\omega||^2 + \\sum_{i=1}^{|X|} \\max(0, 1-y_i \\cdot \\omega^\\top X_i)$. <br> <br> Regularization is done by adding a norm on the parameter vector and including that in the objective function. A shorter parameter vector gives a larger margin for this model. The $l_2$ norm is defined as $\\sqrt{\\sum_{i=1}^n \\omega_i^2}$. The regularization always has some positive attenuation parameter $\\lambda \\in \\mathbb{R}$ keeping it from dominating the objective function. It symbolizes a trade-off between a more accurate classification and wider margins, while also giving the objective function a unique solution.\n",
        "\n",
        "3. __Gradient descent__. The update for gradient descent looks like $\\omega^{(t)} = \\omega^{(t-1)} - \\gamma \\nabla \\mathfrak{L}(\\omega^{(t-1)})$, where the update gradient is defined as $\\nabla \\mathfrak{L} = \\left ( \\frac{\\partial \\mathfrak{L}}{\\partial \\omega_1}, \\frac{\\partial \\mathfrak{L}}{\\partial \\omega_2}, \\ldots, \\frac{\\partial \\mathfrak{L}}{\\partial \\omega_n} \\right )^\\top$. The expression for this gradient $\\nabla \\mathfrak{L}$ is given analytically as: <br> <br> $\\nabla \\mathfrak{L}(X, y, \\omega) = \\lambda \\omega + \\sum_{i=1}^{|X|}\n",
        "\\begin{cases}\n",
        "0 &amp; \\text{if } y_i \\omega^\\top X_i \\geq 1\\\\\n",
        "-y_i X_i &amp; \\text{else}\n",
        "\\end{cases}$\n",
        "\n",
        "In the expression for the gradient, $X_i$ is a vector and $y_i$ is a scalar. These two refer to the i:th data point and its label. The learning rate $\\gamma \\in \\mathbb{R}$ acts as a scaling/dampening factor on the gradient update. This should run until some stopping criteria is met (e.g., $\\omega^{(t+1)}\\approx \\omega^{(t)}$). The default stopping criterion for SGDClassifier is when **_loss_<sub>_current_</sub> &gt; _loss_<sub>_best_</sub>** - .001 for five consecutive iterations.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wget"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgKLp2-e_xnR",
        "outputId": "93a95b53-96af-4301-baec-49db2f87d0d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9657 sha256=dac655d4e6ebe1cca72981ab33640783fa7d81d0f7c681a7c34fa54058b0d4f5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBVZ8YLA_ENq"
      },
      "outputs": [],
      "source": [
        "# Downloading the data\n",
        "\n",
        "import wget\n",
        "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz\"\n",
        "data = wget.download(url)\n",
        "!tar zxf review_polarity.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eG0kkRGs_ENr"
      },
      "outputs": [],
      "source": [
        "# Part 1: Parsing the dataset\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "\n",
        "DEFAULT_FOLDER_NAME = \"txt_sentoken/\"\n",
        "\n",
        "def parse_dataset():\n",
        "    reviews_lst, labels_lst = [], []\n",
        "    current_dir = os.path.join(str(pathlib.Path().absolute()), DEFAULT_FOLDER_NAME)\n",
        "\n",
        "    for folder in os.listdir(current_dir): # 'pos' folder or 'neg' folder\n",
        "        sub_path = os.path.join(current_dir, folder) # Update path\n",
        "        pos = folder == 'pos'  # negative or positive\n",
        "        label = 1 if pos else -1\n",
        "\n",
        "        for file in os.listdir(sub_path):\n",
        "            _sub_path = os.path.join(sub_path, file) # Update path\n",
        "            with open(_sub_path, encoding='utf-8') as f: # Open file(s) with proper encoding\n",
        "                reviews_lst.append(f.read())\n",
        "                labels_lst.append(label)\n",
        "\n",
        "    return reviews_lst, np.asarray(labels_lst)\n",
        "\n",
        "X_raw, y = parse_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAqM9I90_ENr",
        "outputId": "afe6a246-94da-472f-9c77-9d8646a85898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizing the data...\n",
            "Tokenization complete!\n",
            "Populating matrix...\n",
            "Matrix is populated!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import random\n",
        "\n",
        "RE_PATTERN = r\"('(?=\\s)|''|``|--|[&$%,;:.!?\\)\\(\\\"]|\\d+[\\.,]\\d+|\\w+(?=n't)|n't|\\w+(?=')|'\\w+|\\w+\\-\\w+|[A-Z][a-z]+\\.(?=\\s[A-Z])|(?:[A-Za-z]\\.)+|\\w+)\"\n",
        "STOPWORDS = {'.', ',', '?', '\"', '``', \"''\", \"'\", '--', '-', ':', ';', '(', ')', 'i', 'a', 'about', 'after', 'all', 'also', 'an', 'any',\n",
        "             'are', 'as', 'at', 'and', 'be', 'being', 'because', 'been', 'but', 'by', 'can', \"'d\", 'did', 'do', \"don'\", 'don', 'for',\n",
        "             'from', 'had', 'has', 'have', 'he', 'her', 'him', 'his', 'how', 'if', 'is', 'in', 'it', 'its', \"'ll\", \"'m\", 'me',\n",
        "             'more', 'my', 'n', 'of', 'on', 'one', 'or', \"'re\", \"'s\", \"s\", 'said', 'say', 'says', 'she', 'so', 'some', 'such', \"'t\",\n",
        "             'than', 'that', 'the', 'them', 'they', 'their', 'there', 'this', 'to', 'up', 'us', \"'ve\", 'was', 'we', 'were', 'what',\n",
        "             'when', 'where', 'which', 'who', 'will', 'with', 'you', 'your', '&', \"n't\"}\n",
        "\n",
        "ordered_vocabulary = set()\n",
        "\n",
        "print('Tokenizing the data...')\n",
        "for review in X_raw:\n",
        "    tokens = [token.lower() for token in re.findall(RE_PATTERN, review.strip()) if token.lower() not in STOPWORDS]\n",
        "    ordered_vocabulary.update(tokens)\n",
        "\n",
        "ordered_vocabulary = sorted(ordered_vocabulary)\n",
        "print('Tokenization complete!')\n",
        "\n",
        "X = np.zeros((len(y), len(ordered_vocabulary)))\n",
        "\n",
        "print('Populating matrix...')\n",
        "for j, review in enumerate(X_raw):\n",
        "    tokens_in_review = set(token.lower() for token in re.findall(RE_PATTERN, review.strip()) if token.lower() not in STOPWORDS)\n",
        "    X[j, :] = [1 if word in tokens_in_review else 0 for word in ordered_vocabulary]\n",
        "\n",
        "print('Matrix is populated!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "li3BKdKQ_ENs"
      },
      "outputs": [],
      "source": [
        "# Testing 'Part 2: Feature extraction'\n",
        "\n",
        "import numpy as np\n",
        "assert len(X_raw) == 2000\n",
        "assert np.all([isinstance(x, str) for x in X_raw])\n",
        "assert len(X_raw) == y.shape[0]\n",
        "assert len(np.unique(y)) == 2\n",
        "assert y.min() == -1\n",
        "assert y.max() == 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle 'X' and 'y' in the same way\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Reshape 'y'\n",
        "y = y.reshape(-1, 1)  # Use -1 to infer the size of the first dimension automatically\n",
        "\n",
        "# Get training and test sets\n",
        "divisor = int(X.shape[0] * 0.8)\n",
        "X_train, X_test, y_train, y_test = X[:divisor], X[divisor:], y[:divisor], y[divisor:]"
      ],
      "metadata": {
        "id": "1h1TJw3UDXwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part 3: Learning framework\n",
        "\n",
        "class Model:\n",
        "    def __init__(self, lamb, learning_rate, max_iterations=1000):\n",
        "        self.lamb = lamb\n",
        "        self.learning_rate = learning_rate\n",
        "        self.max_iterations = max_iterations\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        weights = np.zeros(n_features)\n",
        "        consecutive, best_loss = 0, 0\n",
        "\n",
        "        while consecutive < 5:\n",
        "            for _ in range(self.max_iterations):\n",
        "                gradients, loss = self.__gradients(weights, X, y)\n",
        "                weights_g = self.lamb * weights + gradients\n",
        "                weights = weights - self.learning_rate * weights_g\n",
        "                if loss > best_loss - 0.001:\n",
        "                    consecutive += 1\n",
        "                best_loss = loss\n",
        "        return weights\n",
        "\n",
        "\n",
        "    def predict(self, X, weight):\n",
        "        prediction_lst = []\n",
        "        for feature in X:\n",
        "            score = np.dot(weight, feature)\n",
        "            if score > 0:\n",
        "                prediction_lst.append(1)\n",
        "            else:\n",
        "                prediction_lst.append(-1)\n",
        "        return np.array(prediction_lst)\n",
        "\n",
        "\n",
        "    def score(self, X, y, weight):\n",
        "        prediction, correct_num = self.predict(X, weight), 0\n",
        "        for index, score in enumerate(prediction):\n",
        "            if score == y[index][0]:\n",
        "                correct_num += 1\n",
        "        return float(correct_num / len(y))\n",
        "\n",
        "\n",
        "    def __gradients(self, weights, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        gradients= np.zeros(n_features)\n",
        "        loss, norm = 0, 0\n",
        "\n",
        "        for j, feature in enumerate(X):\n",
        "            g_step = y[j][0] * np.vdot(weights, feature)\n",
        "            if 1 - g_step > 0:\n",
        "                loss += 1 - g_step\n",
        "            if y[j] * np.vdot(weights, feature) < 1:\n",
        "                gradients += -y[j][0] * feature\n",
        "\n",
        "        for w in weights:\n",
        "            norm += w ** 2\n",
        "        norm = (norm ** 0.5) ** 2\n",
        "        loss += norm * self.lamb / 2\n",
        "\n",
        "        return gradients, loss"
      ],
      "metadata": {
        "id": "I_NVxJFc-bfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Part 3: Learning framework\n",
        "# # Improved Upon Code\n",
        "\n",
        "# class Model:\n",
        "#     def __init__(self, lamb, learning_rate, max_iterations=1000):\n",
        "#         self.lamb = lamb\n",
        "#         self.learning_rate = learning_rate\n",
        "#         self.max_iterations = max_iterations\n",
        "\n",
        "#     def fit(self, X, y):\n",
        "#         n_samples, n_features = X.shape\n",
        "#         weights = np.zeros(n_features)\n",
        "#         consecutive, best_loss = 0, float('inf')\n",
        "\n",
        "#         while consecutive < 5 and self.max_iterations > 0:\n",
        "#             for _ in range(self.max_iterations):\n",
        "#                 gradients, loss = self.__gradients(weights, X, y)\n",
        "#                 weights_g = self.lamb * weights + gradients\n",
        "#                 weights = weights - self.learning_rate * weights_g\n",
        "#                 if best_loss - loss < 0.001:\n",
        "#                     consecutive += 1\n",
        "#                 else:\n",
        "#                     consecutive = 0\n",
        "#                 best_loss = loss\n",
        "#                 self.max_iterations -= 1\n",
        "#         return weights\n",
        "\n",
        "#     def predict(self, X, weight):\n",
        "#         prediction_lst = []\n",
        "#         for feature in X:\n",
        "#             score = np.dot(weight, feature)\n",
        "#             if score > 0:\n",
        "#                 prediction_lst.append(1)\n",
        "#             else:\n",
        "#                 prediction_lst.append(-1)\n",
        "#         return np.array(prediction_lst)\n",
        "\n",
        "#     def score(self, X, y, weight):\n",
        "#         prediction = self.predict(X, weight)\n",
        "#         correct_num = np.sum(prediction == y[:, 0])\n",
        "#         return float(correct_num / len(y))\n",
        "\n",
        "#     def __gradients(self, weights, X, y):\n",
        "#         n_samples, n_features = X.shape\n",
        "#         gradients = np.zeros(n_features)\n",
        "#         loss, norm = 0, 0\n",
        "\n",
        "#         for j, feature in enumerate(X):\n",
        "#             g_step = y[j][0] * np.dot(weights, feature)\n",
        "#             if 1 - g_step > 0:\n",
        "#                 loss += 1 - g_step\n",
        "#             if y[j][0] * np.dot(weights, feature) < 1:\n",
        "#                 gradients += -y[j][0] * feature\n",
        "\n",
        "#         for w in weights:\n",
        "#             norm += w ** 2\n",
        "#         loss += norm * self.lamb / 2\n",
        "\n",
        "#         return gradients, loss"
      ],
      "metadata": {
        "id": "2GDgPv6CFVC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kSqcs7Tz_ENs",
        "outputId": "061bda83-55e7-4f64-bfda-038785b5ece7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning rate: 0.0001, Lambda: 0.0001, Accuracy: 0.8525\n"
          ]
        }
      ],
      "source": [
        "model = Model(0.0001, 0.0001)\n",
        "weight = model.fit(X_train, y_train)\n",
        "score = model.score(X_test, y_test, weight)\n",
        "print(f'Learning rate: {0.0001}, Lambda: {0.0001}, Accuracy: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKM4bVMt_ENs"
      },
      "outputs": [],
      "source": [
        "# # Part 4: Exploring hyperparameters\n",
        "\n",
        "# learning_rate_list = [0.0001, 0.0003] #, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
        "# lamb_list = [0.0001, 0.0003] #, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
        "\n",
        "# for learning_rate in learning_rate_list:\n",
        "#     for lamb in lamb_list:\n",
        "#         model = Model(lamb, learning_rate)\n",
        "#         weight = model.fit(X_train, y_train)\n",
        "#         score = model.score(X_test, y_test, weight)\n",
        "#         print(f'Learning rate: {learning_rate}, Lambda: {lamb}, Accuracy: {score}')\n",
        "#         del model"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}